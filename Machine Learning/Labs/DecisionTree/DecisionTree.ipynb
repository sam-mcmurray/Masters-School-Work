{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### IMPORT THE PACKAGES NUMPY, PANDAS, SCKIT-LEARN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Used the scikit-learn decision tree to compare results, additionally used accuracy score for the metric and made use of this package for scoring the models. The Ordinal encoder was used in order to convert the nominal text values to scores to run in the decision tree."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier as Scikit_DTC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### IMPORT THE IRIS, WINE, AND BREAST-CANCER DATASETS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "# iris dataset\n",
    "dataset1 = pd.read_csv(\"iris.tmls\")\n",
    "# store data type\n",
    "datatype1 = dataset1.iloc[:1, :-1]\n",
    "# remove data type\n",
    "dataset1 = dataset1.iloc[1:, :]\n",
    "# wine dataset\n",
    "dataset2 = pd.read_csv(\"wine.tmls\")\n",
    "# store data type\n",
    "datatype2 = dataset2.iloc[:1, :-1]\n",
    "# remove data type\n",
    "dataset2 = dataset2.iloc[1:, :]\n",
    "# breast-cancer dataset\n",
    "dataset3 = pd.read_csv(\"breast-cancer.tmls\")\n",
    "# save data type\n",
    "datatype3 = dataset3.iloc[:1, 1:]\n",
    "# split dataset for ordinal encoding of the features\n",
    "target3 = dataset3.iloc[1:, 0].reset_index(drop=True)\n",
    "X3 = dataset3.iloc[1:, 1:]\n",
    "oc = OrdinalEncoder()\n",
    "# the encoded X\n",
    "encodedDataset = oc.fit_transform(X3)\n",
    "# convert X from numpy array to pandas dataframe\n",
    "X3 = pd.DataFrame(encodedDataset)\n",
    "# give original feature name back to columns\n",
    "X3 = X3.rename({ 0: 'age', 1: 'menopause', 2: 'tumor-size', 3: 'inv-nodes', 4: 'node-caps', 5: 'deg-malig', 6: 'breast', 7: 'breast-quad', 8: 'irradiate'}, axis=1)\n",
    "# combine X and y\n",
    "dataset3 = pd.concat([X3, target3], axis=1)\n",
    "cv = 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CROSS-VALIDATOR TO SPLIT THE DATASETS INTO EQUAL PARTS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The datasets were tested on 10-fold cross-validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def cross_validator(number: int, size: int, dataset):\n",
    "    parts = []\n",
    "    start = 0\n",
    "    end = size\n",
    "    for i in range(number):\n",
    "        parts.append(dataset.iloc[start:end, :])\n",
    "        start += size\n",
    "        end += size\n",
    "    return parts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### THE IMPURITY CALCULATIONS FUNCTIONS CLASS PROBABILITIES ENTROPY AND GINI INDEX"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "\n",
    "def class_probabilities(target_labels: list[any]) -> list[float]:\n",
    "    # class labels\n",
    "    labels = np.unique(target_labels)\n",
    "    # print(\"Target Labels: \", labels)\n",
    "    # total instances in the current y at this split\n",
    "    total_instances = len(target_labels)\n",
    "    # instantiate the class probabilities list for each of the current labels\n",
    "    class_probs = []\n",
    "    for label in labels:\n",
    "        samples = []\n",
    "        # if the sample contains the same label it is appened to the samples list\n",
    "        samples = [samples.append(sample) for sample in target_labels if label == sample]\n",
    "        # count is the class count of the current label\n",
    "        count = len(samples)\n",
    "        # calculate class probabibilty of the current\n",
    "        class_probability = count/total_instances\n",
    "        # append the class probablility for the list\n",
    "        class_probs.append(class_probability)\n",
    "    return class_probs\n",
    "\n",
    "def calc_entropy(class_probs: list[float]):\n",
    "    return sum(- p * np.log2(p) for p in class_probs if p > 0)\n",
    "\n",
    "def calc_gini_index(class_probs: list[float]):\n",
    "    return sum(2 * p * (1 - p) for p in class_probs if p > 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NODE CLASS USED WITHIN DECISION TREE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, dType=None, *, value=None):\n",
    "        # store feature of the split\n",
    "        self.feature = feature\n",
    "        # store the threshold for splitting to left or right branch\n",
    "        self.threshold = threshold\n",
    "        # store the left node/leaf\n",
    "        self.left = left\n",
    "        # store the right node/leaf\n",
    "        self.right = right\n",
    "        # store the data type nominal or real\n",
    "        self.dType = dType\n",
    "        # value if leaf\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf(self):\n",
    "        # check to see if is a leaf\n",
    "        return self.value is not None\n",
    "\n",
    "    def print_node(self):\n",
    "        # print node used for debugging\n",
    "        if self.is_leaf():\n",
    "            print(\"Leaf Value: \", self.value)\n",
    "        else:\n",
    "            print(\"Branch: \")\n",
    "            print(\"Feature: \", self.feature)\n",
    "            print(\"Threshold: \", self.threshold)\n",
    "            print(\"Left: \", self.left)\n",
    "            print(\"Right: \", self.right)\n",
    "            print(\"Data Type: \", self.dType)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DECISION TREE CLASSIFIER CLASS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, purity_function = 'gini', max_depth=50):\n",
    "        # set the purity function\n",
    "        self.purity_function = purity_function\n",
    "        # set max depth of the tree to handle overfitting\n",
    "        self.max_depth = max_depth\n",
    "        # the root node of the tree\n",
    "        self.root = None\n",
    "\n",
    "    def impurity(self, y):\n",
    "        # list of calculated class probabilities of y\n",
    "        class_probs = class_probabilities(y)\n",
    "        # initialized purity\n",
    "        purity = -1\n",
    "        if self.purity_function == 'gini':\n",
    "            # function call for the defualt gini index purity function\n",
    "            purity = calc_gini_index(class_probs)\n",
    "        else:\n",
    "            # function call for entropy (not tested)\n",
    "            purity = calc_entropy(class_probs)\n",
    "        return purity\n",
    "\n",
    "    def purity_gain(self, X_column, y, split_threshold, datatype):\n",
    "        # calculate the Information Gain over the parent and child node\n",
    "        # parents impurity\n",
    "        d_parent = self.impurity(y)\n",
    "        # split the values based on the threshold with split function\n",
    "        left, right = self.split(X_column, split_threshold, datatype)\n",
    "        # if the length of one of the splits was 0 then this would be pure\n",
    "        if len(left) == 0 or len(right) == 0:\n",
    "            return 0\n",
    "        # length of all from the parent node\n",
    "        d = len(y)\n",
    "        # length of left or right child\n",
    "        l, r = len(left), len(right)\n",
    "        # impurity of left and right\n",
    "        d_l, d_r = self.impurity(y.iloc[left]), self.impurity(y.iloc[right])\n",
    "        # the purity of the children\n",
    "        d_child = (l / d) * d_l + (r / d) * d_r\n",
    "        return d_parent - d_child\n",
    "\n",
    "\n",
    "    def homogeneous(self, y)-> bool:\n",
    "        # function to check if the y values are homogeneous\n",
    "        # get the class counts\n",
    "        unique_classes, class_count = self.class_counts(y)\n",
    "        # if the number of unique classes are one returns True\n",
    "        # so that a leaf node with the label can be created\n",
    "        if len(unique_classes) == 1:\n",
    "            return True\n",
    "        n_samples = y.shape[0]\n",
    "        majority_value = 0.0\n",
    "        # if the classes are not 100% homogeneous then check each value\n",
    "        for index, cl in enumerate(unique_classes):\n",
    "            # current class count\n",
    "            current_class_count = class_count[index]\n",
    "            # class proportion\n",
    "            proportion = current_class_count / n_samples\n",
    "            # if proportion is better than the previous majority value\n",
    "            if proportion > majority_value:\n",
    "                majority_value = proportion\n",
    "        # the primary threshold for homgeneousness is 95%\n",
    "        # if the majority values are greater than 95% then it would return true\n",
    "        if majority_value > 0.95:\n",
    "            return True\n",
    "        # a check needs to be made so that if a problem exists where a node finishes with a tie below 5 samples\n",
    "        elif n_samples < 5:\n",
    "            if majority_value >= .50:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def label(self, y):\n",
    "        # label function is used when y contains a homogeneous amount the label is assigned to the leaf\n",
    "        # get unique classes from the class count function\n",
    "        unique_classes, class_counts = self.class_counts(y)\n",
    "        # if unique classes is 1 then returns the unique class label\n",
    "        if len(unique_classes) == 1:\n",
    "            return unique_classes[0]\n",
    "        # get y sample size\n",
    "        n_samples = y.shape[0]\n",
    "        # instantiate majority value\n",
    "        majority_value = 0.0\n",
    "        # instantiate majority class\n",
    "        majority_class = ''\n",
    "        # for each class get the context and proportion to the total number of samples\n",
    "        for index, cl in enumerate(unique_classes):\n",
    "            # count of the current class\n",
    "            current_class_count = class_counts[index]\n",
    "            # proportion of current class\n",
    "            proportion = current_class_count / n_samples\n",
    "            # if the proportion is greater than the current majority value\n",
    "            if proportion > majority_value:\n",
    "                # set majority value\n",
    "                majority_value = proportion\n",
    "                # set majority class\n",
    "                majority_class = cl\n",
    "        return majority_class\n",
    "\n",
    "    def traverse_tree(self, x, node):\n",
    "        # if node is a leaf return the value or the prediction\n",
    "        if node.is_leaf():\n",
    "            # prediction\n",
    "            return node.value\n",
    "        # if node dtype or data type is nominal or 'n'\n",
    "        if node.dType == 'n':\n",
    "            # if the feature value of x is equal to the threshold it will be the left child\n",
    "            # if the value is not equal it will be in the right child\n",
    "            if x[node.feature].values == node.threshold.astype(float):\n",
    "                return self.traverse_tree(x, node.left)\n",
    "            return self.traverse_tree(x, node.right)\n",
    "        # if node dtype or data type is real value or 'r' and if the feature value of x is\n",
    "        # less than or equal to the threshold it will be the left child, if the value is not\n",
    "        # equal it will be in the right child\n",
    "        elif node.dType == 'r' and x[node.feature].values <= node.threshold.astype(float):\n",
    "            return self.traverse_tree(x, node.left)\n",
    "        return self.traverse_tree(x, node.right)\n",
    "\n",
    "    def best_split(self, X, y, n_features, datatype):\n",
    "        # features from X\n",
    "        features = X.columns\n",
    "        # initialize best gain\n",
    "        best_gain = -1\n",
    "        # initialize best threshold\n",
    "        best_threshold = 0.0\n",
    "        # initialize best feature\n",
    "        best_feature = ''\n",
    "        # initialize data type\n",
    "        dtype = ''\n",
    "        # for every feature\n",
    "        for feature in range(n_features):\n",
    "            # get the data type\n",
    "            dtype = datatype.iloc[:, feature].values\n",
    "            # check if the data type is real values or 'r'\n",
    "            if dtype == 'r':\n",
    "                # get the feature column\n",
    "                X_column = X.iloc[:, feature]\n",
    "                # set the threshold as the mean of the feature column\n",
    "                threshold = X_column.mean(axis=0)\n",
    "                # get the gain from the purity gain function\n",
    "                gain = self.purity_gain(X_column, y, threshold, dtype)\n",
    "                # if the current gain is better apply values for change\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_threshold = threshold\n",
    "                    best_feature = features[feature]\n",
    "            # check if the data type is nominal values or 'n'\n",
    "            if dtype == 'n':\n",
    "                # get the feature column\n",
    "                X_column = X.iloc[:, feature]\n",
    "                # get the nominal values from the unique values within dataframe\n",
    "                nominal_values = np.unique(X_column)\n",
    "                # each nominal value is used as a threshold\n",
    "                for threshold in nominal_values:\n",
    "                    # gain of this nominal value as the threshold\n",
    "                    gain = self.purity_gain(X_column, y, threshold, dtype)\n",
    "                    # if the current gain is better apply values for change\n",
    "                    if gain > best_gain:\n",
    "                        best_gain = gain\n",
    "                        best_threshold = threshold\n",
    "                        best_feature = features[feature]\n",
    "            # the best feature, best threshold and data type are returned\n",
    "        return best_feature, best_threshold, dtype\n",
    "\n",
    "    def split(self, X, threshold, datatype):\n",
    "        # the split function that handles the split and returns the indices\n",
    "        # if the data type is 'r' or real values\n",
    "        if datatype == 'r':\n",
    "            # performs the threshold split based left split as values of less\n",
    "            # than or equal to while those that are greater split to the right\n",
    "            # indices\n",
    "            left_indices = np.argwhere(X.values <= threshold).flatten()\n",
    "            right_indices = np.argwhere(X.values > threshold).flatten()\n",
    "        # if the data type is 'n' or nominal values\n",
    "        if datatype == 'n':\n",
    "            # performs the threshold split based left indices as values that are\n",
    "            # equal to the threshold while all others would split to the right\n",
    "            # indices\n",
    "            left_indices = np.argwhere(X.values == threshold).flatten()\n",
    "            right_indices = np.argwhere(X.values != threshold).flatten()\n",
    "        return left_indices, right_indices\n",
    "\n",
    "\n",
    "    def class_counts(self, y):\n",
    "        # function that performs class count\n",
    "        # get all unique values in the y or target feature\n",
    "        unique_classes = np.unique(y)\n",
    "        # initialize class counts to store all class counts\n",
    "        class_counts = []\n",
    "        # for every unique class samples are counted\n",
    "        for cl in unique_classes:\n",
    "            # current class samples are added to a list for a count of length\n",
    "            # append to the list\n",
    "            class_samples = []\n",
    "            class_samples = [class_samples.append(sample) for sample in y if cl == sample]\n",
    "            current_class_count = len(class_samples)\n",
    "            class_counts.append(current_class_count)\n",
    "        # the list of the unique classes as well as there counts are returned\n",
    "        return unique_classes, class_counts\n",
    "\n",
    "    def fit(self, X, y, datatype):\n",
    "        # the start of the tree that will be based on the splits from the data\n",
    "        self.root = self.grow_tree(X, y, datatype)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        # function for the prediction\n",
    "        #initialize list for predictions\n",
    "        predictions = []\n",
    "        # for each x in X traverse tree to find a label\n",
    "        for index in range(len(X)):\n",
    "            # split into x\n",
    "            x = X.iloc[[index]]\n",
    "            # traverse tree function returning the label from the traversal\n",
    "            prediction = self.traverse_tree(x, self.root)\n",
    "            # add prediction to the list of predictions\n",
    "            predictions.append(prediction)\n",
    "        # predictions for all x are returned\n",
    "        return predictions\n",
    "\n",
    "    def grow_tree(self, X, y, datatype, depth=0):\n",
    "        # grow tree function grows the tree recursivly\n",
    "        # reset the index of the X and y\n",
    "        X = X.reset_index(drop=True)\n",
    "        y = y.reset_index(drop=True)\n",
    "        # number of samples and number of features\n",
    "        n_samples, n_features = X.shape\n",
    "        # check if the current node is already homogeneous also to see if depth\n",
    "        if self.homogeneous(y) or depth > self.max_depth:\n",
    "            # create a leaf with the value of the label\n",
    "            leaf = Node(value=self.label(y))\n",
    "            return leaf\n",
    "        # the best split function returns the best feature, best threshold, and the data tyoe\n",
    "        best_feature, best_threshold, d_type = self.best_split(X, y, n_features, datatype)\n",
    "        # the best feature are used on the final split\n",
    "        left, right = self.split(X[best_feature], best_threshold, d_type)\n",
    "        # if the length of left and right are not equal to 0 the tree grows from this node\n",
    "        # via left or right child\n",
    "        if len(left) != 0:\n",
    "            # left child of the current node\n",
    "            left = self.grow_tree(X.iloc[left, :], y.iloc[left], datatype, depth + 1)\n",
    "        if len(right) != 0:\n",
    "            # right child of the current node\n",
    "            right = self.grow_tree(X.iloc[right, :], y.iloc[right], datatype, depth + 1)\n",
    "\n",
    "        return Node(best_feature, best_threshold, left, right, d_type)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GET SIZES FOR TRAIN AND TEST FOR EACH OF THE DATASETS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "# iris dataset\n",
    "samples1, features1 = dataset1.shape\n",
    "test_size1 = samples1 // cv\n",
    "train_size1 = samples1 - test_size1\n",
    "# Wine dataset\n",
    "samples2, features2 = dataset2.shape\n",
    "test_size2 = samples2 // cv\n",
    "train_size2 = samples2 - test_size2\n",
    "# breast-cancer dataset\n",
    "samples3, features3 = dataset3.shape\n",
    "test_size3 = samples3 // cv\n",
    "train_size3 = samples3 - test_size3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### IRIS DATASET 10 X 10-FOLD CROSS VALIADATION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Dataset\n",
      "sklearn fold  0  accuracy:  1.0\n",
      "My DT Fold 0  accuracy: 1.0\n",
      "sklearn fold  1  accuracy:  0.8666666666666667\n",
      "My DT Fold 1  accuracy: 0.8666666666666667\n",
      "sklearn fold  2  accuracy:  0.9333333333333333\n",
      "My DT Fold 2  accuracy: 0.9333333333333333\n",
      "sklearn fold  3  accuracy:  1.0\n",
      "My DT Fold 3  accuracy: 1.0\n",
      "sklearn fold  4  accuracy:  1.0\n",
      "My DT Fold 4  accuracy: 1.0\n",
      "sklearn fold  5  accuracy:  0.9333333333333333\n",
      "My DT Fold 5  accuracy: 0.9333333333333333\n",
      "sklearn fold  6  accuracy:  1.0\n",
      "My DT Fold 6  accuracy: 1.0\n",
      "sklearn fold  7  accuracy:  0.9333333333333333\n",
      "My DT Fold 7  accuracy: 0.9333333333333333\n",
      "sklearn fold  8  accuracy:  0.9333333333333333\n",
      "My DT Fold 8  accuracy: 0.9333333333333333\n",
      "sklearn fold  9  accuracy:  0.9333333333333333\n",
      "My DT Fold 9  accuracy: 0.8\n",
      "sklearn fold  0  accuracy:  0.9333333333333333\n",
      "My DT Fold 0  accuracy: 1.0\n",
      "sklearn fold  1  accuracy:  0.9333333333333333\n",
      "My DT Fold 1  accuracy: 0.9333333333333333\n",
      "sklearn fold  2  accuracy:  0.9333333333333333\n",
      "My DT Fold 2  accuracy: 0.8666666666666667\n",
      "sklearn fold  3  accuracy:  1.0\n",
      "My DT Fold 3  accuracy: 1.0\n",
      "sklearn fold  4  accuracy:  1.0\n",
      "My DT Fold 4  accuracy: 1.0\n",
      "sklearn fold  5  accuracy:  0.9333333333333333\n",
      "My DT Fold 5  accuracy: 0.8666666666666667\n",
      "sklearn fold  6  accuracy:  1.0\n",
      "My DT Fold 6  accuracy: 0.9333333333333333\n",
      "sklearn fold  7  accuracy:  0.9333333333333333\n",
      "My DT Fold 7  accuracy: 0.9333333333333333\n",
      "sklearn fold  8  accuracy:  1.0\n",
      "My DT Fold 8  accuracy: 1.0\n",
      "sklearn fold  9  accuracy:  0.8666666666666667\n",
      "My DT Fold 9  accuracy: 0.9333333333333333\n",
      "sklearn fold  0  accuracy:  0.9333333333333333\n",
      "My DT Fold 0  accuracy: 0.9333333333333333\n",
      "sklearn fold  1  accuracy:  1.0\n",
      "My DT Fold 1  accuracy: 1.0\n",
      "sklearn fold  2  accuracy:  0.9333333333333333\n",
      "My DT Fold 2  accuracy: 0.9333333333333333\n",
      "sklearn fold  3  accuracy:  0.9333333333333333\n",
      "My DT Fold 3  accuracy: 0.9333333333333333\n",
      "sklearn fold  4  accuracy:  0.9333333333333333\n",
      "My DT Fold 4  accuracy: 0.9333333333333333\n",
      "sklearn fold  5  accuracy:  0.9333333333333333\n",
      "My DT Fold 5  accuracy: 0.9333333333333333\n",
      "sklearn fold  6  accuracy:  1.0\n",
      "My DT Fold 6  accuracy: 0.9333333333333333\n",
      "sklearn fold  7  accuracy:  0.8666666666666667\n",
      "My DT Fold 7  accuracy: 0.8666666666666667\n",
      "sklearn fold  8  accuracy:  1.0\n",
      "My DT Fold 8  accuracy: 1.0\n",
      "sklearn fold  9  accuracy:  0.9333333333333333\n",
      "My DT Fold 9  accuracy: 0.9333333333333333\n",
      "sklearn fold  0  accuracy:  1.0\n",
      "My DT Fold 0  accuracy: 1.0\n",
      "sklearn fold  1  accuracy:  1.0\n",
      "My DT Fold 1  accuracy: 1.0\n",
      "sklearn fold  2  accuracy:  0.9333333333333333\n",
      "My DT Fold 2  accuracy: 0.9333333333333333\n",
      "sklearn fold  3  accuracy:  1.0\n",
      "My DT Fold 3  accuracy: 1.0\n",
      "sklearn fold  4  accuracy:  0.9333333333333333\n",
      "My DT Fold 4  accuracy: 1.0\n",
      "sklearn fold  5  accuracy:  1.0\n",
      "My DT Fold 5  accuracy: 1.0\n",
      "sklearn fold  6  accuracy:  0.9333333333333333\n",
      "My DT Fold 6  accuracy: 0.8666666666666667\n",
      "sklearn fold  7  accuracy:  1.0\n",
      "My DT Fold 7  accuracy: 0.9333333333333333\n",
      "sklearn fold  8  accuracy:  0.8666666666666667\n",
      "My DT Fold 8  accuracy: 0.8666666666666667\n",
      "sklearn fold  9  accuracy:  0.8666666666666667\n",
      "My DT Fold 9  accuracy: 0.8666666666666667\n",
      "sklearn fold  0  accuracy:  0.9333333333333333\n",
      "My DT Fold 0  accuracy: 0.9333333333333333\n",
      "sklearn fold  1  accuracy:  1.0\n",
      "My DT Fold 1  accuracy: 1.0\n",
      "sklearn fold  2  accuracy:  0.9333333333333333\n",
      "My DT Fold 2  accuracy: 0.8666666666666667\n",
      "sklearn fold  3  accuracy:  0.8666666666666667\n",
      "My DT Fold 3  accuracy: 0.9333333333333333\n",
      "sklearn fold  4  accuracy:  0.9333333333333333\n",
      "My DT Fold 4  accuracy: 0.8\n",
      "sklearn fold  5  accuracy:  0.9333333333333333\n",
      "My DT Fold 5  accuracy: 0.9333333333333333\n",
      "sklearn fold  6  accuracy:  1.0\n",
      "My DT Fold 6  accuracy: 1.0\n",
      "sklearn fold  7  accuracy:  1.0\n",
      "My DT Fold 7  accuracy: 0.9333333333333333\n",
      "sklearn fold  8  accuracy:  0.9333333333333333\n",
      "My DT Fold 8  accuracy: 0.9333333333333333\n",
      "sklearn fold  9  accuracy:  0.8666666666666667\n",
      "My DT Fold 9  accuracy: 0.8666666666666667\n",
      "sklearn fold  0  accuracy:  0.9333333333333333\n",
      "My DT Fold 0  accuracy: 0.9333333333333333\n",
      "sklearn fold  1  accuracy:  1.0\n",
      "My DT Fold 1  accuracy: 1.0\n",
      "sklearn fold  2  accuracy:  0.9333333333333333\n",
      "My DT Fold 2  accuracy: 0.9333333333333333\n",
      "sklearn fold  3  accuracy:  0.8666666666666667\n",
      "My DT Fold 3  accuracy: 0.8666666666666667\n",
      "sklearn fold  4  accuracy:  0.9333333333333333\n",
      "My DT Fold 4  accuracy: 0.8\n",
      "sklearn fold  5  accuracy:  0.9333333333333333\n",
      "My DT Fold 5  accuracy: 1.0\n",
      "sklearn fold  6  accuracy:  1.0\n",
      "My DT Fold 6  accuracy: 1.0\n",
      "sklearn fold  7  accuracy:  1.0\n",
      "My DT Fold 7  accuracy: 1.0\n",
      "sklearn fold  8  accuracy:  0.8\n",
      "My DT Fold 8  accuracy: 0.9333333333333333\n",
      "sklearn fold  9  accuracy:  1.0\n",
      "My DT Fold 9  accuracy: 1.0\n",
      "sklearn fold  0  accuracy:  1.0\n",
      "My DT Fold 0  accuracy: 1.0\n",
      "sklearn fold  1  accuracy:  0.9333333333333333\n",
      "My DT Fold 1  accuracy: 0.9333333333333333\n",
      "sklearn fold  2  accuracy:  1.0\n",
      "My DT Fold 2  accuracy: 0.9333333333333333\n",
      "sklearn fold  3  accuracy:  0.9333333333333333\n",
      "My DT Fold 3  accuracy: 0.9333333333333333\n",
      "sklearn fold  4  accuracy:  0.9333333333333333\n",
      "My DT Fold 4  accuracy: 1.0\n",
      "sklearn fold  5  accuracy:  0.8666666666666667\n",
      "My DT Fold 5  accuracy: 0.9333333333333333\n",
      "sklearn fold  6  accuracy:  0.9333333333333333\n",
      "My DT Fold 6  accuracy: 0.9333333333333333\n",
      "sklearn fold  7  accuracy:  0.8666666666666667\n",
      "My DT Fold 7  accuracy: 0.8666666666666667\n",
      "sklearn fold  8  accuracy:  0.8666666666666667\n",
      "My DT Fold 8  accuracy: 0.8666666666666667\n",
      "sklearn fold  9  accuracy:  1.0\n",
      "My DT Fold 9  accuracy: 1.0\n",
      "sklearn fold  0  accuracy:  0.9333333333333333\n",
      "My DT Fold 0  accuracy: 0.9333333333333333\n",
      "sklearn fold  1  accuracy:  0.9333333333333333\n",
      "My DT Fold 1  accuracy: 0.9333333333333333\n",
      "sklearn fold  2  accuracy:  0.8666666666666667\n",
      "My DT Fold 2  accuracy: 0.8666666666666667\n",
      "sklearn fold  3  accuracy:  0.9333333333333333\n",
      "My DT Fold 3  accuracy: 1.0\n",
      "sklearn fold  4  accuracy:  0.8666666666666667\n",
      "My DT Fold 4  accuracy: 0.9333333333333333\n",
      "sklearn fold  5  accuracy:  0.9333333333333333\n",
      "My DT Fold 5  accuracy: 0.9333333333333333\n",
      "sklearn fold  6  accuracy:  1.0\n",
      "My DT Fold 6  accuracy: 0.9333333333333333\n",
      "sklearn fold  7  accuracy:  1.0\n",
      "My DT Fold 7  accuracy: 1.0\n",
      "sklearn fold  8  accuracy:  0.8666666666666667\n",
      "My DT Fold 8  accuracy: 0.9333333333333333\n",
      "sklearn fold  9  accuracy:  1.0\n",
      "My DT Fold 9  accuracy: 1.0\n",
      "sklearn fold  0  accuracy:  0.9333333333333333\n",
      "My DT Fold 0  accuracy: 0.9333333333333333\n",
      "sklearn fold  1  accuracy:  0.9333333333333333\n",
      "My DT Fold 1  accuracy: 0.8666666666666667\n",
      "sklearn fold  2  accuracy:  0.8666666666666667\n",
      "My DT Fold 2  accuracy: 0.9333333333333333\n",
      "sklearn fold  3  accuracy:  1.0\n",
      "My DT Fold 3  accuracy: 0.9333333333333333\n",
      "sklearn fold  4  accuracy:  0.9333333333333333\n",
      "My DT Fold 4  accuracy: 0.9333333333333333\n",
      "sklearn fold  5  accuracy:  0.9333333333333333\n",
      "My DT Fold 5  accuracy: 1.0\n",
      "sklearn fold  6  accuracy:  1.0\n",
      "My DT Fold 6  accuracy: 0.9333333333333333\n",
      "sklearn fold  7  accuracy:  0.9333333333333333\n",
      "My DT Fold 7  accuracy: 0.9333333333333333\n",
      "sklearn fold  8  accuracy:  0.9333333333333333\n",
      "My DT Fold 8  accuracy: 0.9333333333333333\n",
      "sklearn fold  9  accuracy:  0.9333333333333333\n",
      "My DT Fold 9  accuracy: 0.9333333333333333\n",
      "sklearn fold  0  accuracy:  1.0\n",
      "My DT Fold 0  accuracy: 0.9333333333333333\n",
      "sklearn fold  1  accuracy:  1.0\n",
      "My DT Fold 1  accuracy: 1.0\n",
      "sklearn fold  2  accuracy:  0.9333333333333333\n",
      "My DT Fold 2  accuracy: 0.9333333333333333\n",
      "sklearn fold  3  accuracy:  1.0\n",
      "My DT Fold 3  accuracy: 1.0\n",
      "sklearn fold  4  accuracy:  0.8\n",
      "My DT Fold 4  accuracy: 0.8666666666666667\n",
      "sklearn fold  5  accuracy:  1.0\n",
      "My DT Fold 5  accuracy: 0.9333333333333333\n",
      "sklearn fold  6  accuracy:  0.9333333333333333\n",
      "My DT Fold 6  accuracy: 0.9333333333333333\n",
      "sklearn fold  7  accuracy:  0.8666666666666667\n",
      "My DT Fold 7  accuracy: 0.8666666666666667\n",
      "sklearn fold  8  accuracy:  0.9333333333333333\n",
      "My DT Fold 8  accuracy: 0.9333333333333333\n",
      "sklearn fold  9  accuracy:  1.0\n",
      "My DT Fold 9  accuracy: 1.0\n",
      "sklearn Decision Tree accuracy score test  0 :  0.9533333333333334\n",
      "sklearn Decision Tree accuracy score test  1 :  0.9533333333333334\n",
      "sklearn Decision Tree accuracy score test  2 :  0.9466666666666667\n",
      "sklearn Decision Tree accuracy score test  3 :  0.9533333333333335\n",
      "sklearn Decision Tree accuracy score test  4 :  0.9400000000000001\n",
      "sklearn Decision Tree accuracy score test  5 :  0.9400000000000001\n",
      "sklearn Decision Tree accuracy score test  6 :  0.9333333333333333\n",
      "sklearn Decision Tree accuracy score test  7 :  0.9333333333333333\n",
      "sklearn Decision Tree accuracy score test  8 :  0.9400000000000001\n",
      "sklearn Decision Tree accuracy score test  9 :  0.9466666666666667\n",
      "****************************************************************************************************\n",
      "My Decision Tree accuracy score test  0 :  0.9400000000000001\n",
      "My Decision Tree accuracy score test  1 :  0.9466666666666667\n",
      "My Decision Tree accuracy score test  2 :  0.9400000000000001\n",
      "My Decision Tree accuracy score test  3 :  0.9466666666666667\n",
      "My Decision Tree accuracy score test  4 :  0.9200000000000002\n",
      "My Decision Tree accuracy score test  5 :  0.9466666666666667\n",
      "My Decision Tree accuracy score test  6 :  0.9400000000000001\n",
      "My Decision Tree accuracy score test  7 :  0.9466666666666667\n",
      "My Decision Tree accuracy score test  8 :  0.9333333333333333\n",
      "My Decision Tree accuracy score test  9 :  0.9400000000000001\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "#Iris dataset\n",
    "print(\"Iris Dataset\")\n",
    "# initialize sklearn test scores list\n",
    "sklearn_test_scores = []\n",
    "# initialize my decision tree test scores\n",
    "my_dt_test_scores = []\n",
    "# 10 different tests on a 10 fold cross validation\n",
    "for times in range(10):\n",
    "    # initialize current accuracy lists shuffle dataset\n",
    "    sklearn_accuracy = []\n",
    "    my_accuracy = []\n",
    "    dataset1 = dataset1.sample(frac=1)\n",
    "    dataset1 = dataset1.sample(frac=1)\n",
    "    # split into 10 parts\n",
    "    parts1 = cross_validator(cv, test_size1, dataset1)\n",
    "    for fold in range(cv):\n",
    "        # initialize scikit learn decision tree classifier\n",
    "        scikitDT = Scikit_DTC()\n",
    "        # iniitalize the decision tree classifie\n",
    "        dt = DecisionTreeClassifier()\n",
    "        # test part for the set on particular fold\n",
    "        test = parts1[fold]\n",
    "        # combine all parts except for the test part\n",
    "        train_list  = [parts1[train_part] for train_part in range(cv) if fold != train_part]\n",
    "        # train set\n",
    "        train = pd.concat(train_list)\n",
    "        # split into X for the features and y for target class for both test and train sets\n",
    "        X_test = test.iloc[:, :-1].astype(float).reset_index(drop=True)\n",
    "        y_test = test.iloc[:, -1].reset_index(drop=True)\n",
    "        # both X train and X test sets have the datatype set to float so that numpy can operate\n",
    "        X_train = train.iloc[:, :-1].astype(float).reset_index(drop=True)\n",
    "        y_train = train.iloc[:, -1].reset_index(drop=True)\n",
    "        # scikit learn fits the model then predics returning a list of predictions for y\n",
    "        # adds the reults to scikit-learn list\n",
    "        scikitDT.fit(X_train, y_train)\n",
    "        sky_pred = scikitDT.predict(X_test)\n",
    "        sklearn_acc = accuracy_score(sky_pred, y_test)\n",
    "        print(\"sklearn fold \", fold, \" accuracy: \", sklearn_acc)\n",
    "        sklearn_accuracy.append(sklearn_acc)\n",
    "        # the decision tree classifier train\n",
    "        dt.fit(X_train, y_train, datatype1)\n",
    "        # the decision tree classifier predictions\n",
    "        y_pred = dt.predict(X_test)\n",
    "        # append score to mydt scores list\n",
    "        my_acc = accuracy_score(y_pred, y_test)\n",
    "        my_accuracy.append(my_acc)\n",
    "        print(\"My DT Fold\", fold, \" accuracy:\", my_acc)\n",
    "    # calculate average of 10 for cross validation\n",
    "    sklearn_avg_accuracy = np.array(sklearn_accuracy).mean(axis=0)\n",
    "    # add to overall scores list\n",
    "    sklearn_test_scores.append(sklearn_avg_accuracy)\n",
    "    # calculate average and store in overall scores list\n",
    "    my_avg_accuracy = np.array(my_accuracy).mean(axis=0)\n",
    "    my_dt_test_scores.append(my_avg_accuracy)\n",
    "\n",
    "for i in range(len(sklearn_test_scores)):\n",
    "    print(\"sklearn Decision Tree accuracy score test \", i, \": \", sklearn_test_scores[i])\n",
    "print(\"*\" * 100)\n",
    "\n",
    "for i in range(len(my_dt_test_scores)):\n",
    "    print(\"My Decision Tree accuracy score test \", i, \": \", my_dt_test_scores[i])\n",
    "print(\"*\" * 100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### WINE DATASET 10 X 10-FOLD CROSS VALIADATION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wine Dataset\n",
      "sklearn fold  0  accuracy:  0.8823529411764706\n",
      "My DT Fold 0  accuracy: 0.9411764705882353\n",
      "sklearn fold  1  accuracy:  0.8235294117647058\n",
      "My DT Fold 1  accuracy: 0.8823529411764706\n",
      "sklearn fold  2  accuracy:  1.0\n",
      "My DT Fold 2  accuracy: 0.8235294117647058\n",
      "sklearn fold  3  accuracy:  1.0\n",
      "My DT Fold 3  accuracy: 1.0\n",
      "sklearn fold  4  accuracy:  0.9411764705882353\n",
      "My DT Fold 4  accuracy: 0.7647058823529411\n",
      "sklearn fold  5  accuracy:  0.9411764705882353\n",
      "My DT Fold 5  accuracy: 0.7058823529411765\n",
      "sklearn fold  6  accuracy:  1.0\n",
      "My DT Fold 6  accuracy: 0.9411764705882353\n",
      "sklearn fold  7  accuracy:  0.8235294117647058\n",
      "My DT Fold 7  accuracy: 0.9411764705882353\n",
      "sklearn fold  8  accuracy:  0.8235294117647058\n",
      "My DT Fold 8  accuracy: 0.9411764705882353\n",
      "sklearn fold  9  accuracy:  1.0\n",
      "My DT Fold 9  accuracy: 1.0\n",
      "sklearn fold  0  accuracy:  0.8823529411764706\n",
      "My DT Fold 0  accuracy: 1.0\n",
      "sklearn fold  1  accuracy:  1.0\n",
      "My DT Fold 1  accuracy: 1.0\n",
      "sklearn fold  2  accuracy:  0.8235294117647058\n",
      "My DT Fold 2  accuracy: 0.8235294117647058\n",
      "sklearn fold  3  accuracy:  0.9411764705882353\n",
      "My DT Fold 3  accuracy: 1.0\n",
      "sklearn fold  4  accuracy:  0.9411764705882353\n",
      "My DT Fold 4  accuracy: 0.9411764705882353\n",
      "sklearn fold  5  accuracy:  0.9411764705882353\n",
      "My DT Fold 5  accuracy: 0.9411764705882353\n",
      "sklearn fold  6  accuracy:  0.8823529411764706\n",
      "My DT Fold 6  accuracy: 0.9411764705882353\n",
      "sklearn fold  7  accuracy:  0.8823529411764706\n",
      "My DT Fold 7  accuracy: 0.8235294117647058\n",
      "sklearn fold  8  accuracy:  0.9411764705882353\n",
      "My DT Fold 8  accuracy: 0.9411764705882353\n",
      "sklearn fold  9  accuracy:  0.9411764705882353\n",
      "My DT Fold 9  accuracy: 0.8823529411764706\n",
      "sklearn fold  0  accuracy:  0.9411764705882353\n",
      "My DT Fold 0  accuracy: 0.8235294117647058\n",
      "sklearn fold  1  accuracy:  0.9411764705882353\n",
      "My DT Fold 1  accuracy: 0.8823529411764706\n",
      "sklearn fold  2  accuracy:  0.8823529411764706\n",
      "My DT Fold 2  accuracy: 0.8823529411764706\n",
      "sklearn fold  3  accuracy:  1.0\n",
      "My DT Fold 3  accuracy: 1.0\n",
      "sklearn fold  4  accuracy:  0.8235294117647058\n",
      "My DT Fold 4  accuracy: 0.8823529411764706\n",
      "sklearn fold  5  accuracy:  1.0\n",
      "My DT Fold 5  accuracy: 1.0\n",
      "sklearn fold  6  accuracy:  0.9411764705882353\n",
      "My DT Fold 6  accuracy: 0.8235294117647058\n",
      "sklearn fold  7  accuracy:  0.8823529411764706\n",
      "My DT Fold 7  accuracy: 0.8235294117647058\n",
      "sklearn fold  8  accuracy:  0.9411764705882353\n",
      "My DT Fold 8  accuracy: 0.8235294117647058\n",
      "sklearn fold  9  accuracy:  0.8235294117647058\n",
      "My DT Fold 9  accuracy: 0.8235294117647058\n",
      "sklearn fold  0  accuracy:  0.9411764705882353\n",
      "My DT Fold 0  accuracy: 0.9411764705882353\n",
      "sklearn fold  1  accuracy:  0.9411764705882353\n",
      "My DT Fold 1  accuracy: 0.8235294117647058\n",
      "sklearn fold  2  accuracy:  0.9411764705882353\n",
      "My DT Fold 2  accuracy: 0.9411764705882353\n",
      "sklearn fold  3  accuracy:  1.0\n",
      "My DT Fold 3  accuracy: 0.8823529411764706\n",
      "sklearn fold  4  accuracy:  0.9411764705882353\n",
      "My DT Fold 4  accuracy: 0.9411764705882353\n",
      "sklearn fold  5  accuracy:  0.9411764705882353\n",
      "My DT Fold 5  accuracy: 0.8823529411764706\n",
      "sklearn fold  6  accuracy:  0.8823529411764706\n",
      "My DT Fold 6  accuracy: 0.9411764705882353\n",
      "sklearn fold  7  accuracy:  0.9411764705882353\n",
      "My DT Fold 7  accuracy: 0.8823529411764706\n",
      "sklearn fold  8  accuracy:  0.8235294117647058\n",
      "My DT Fold 8  accuracy: 0.9411764705882353\n",
      "sklearn fold  9  accuracy:  0.8235294117647058\n",
      "My DT Fold 9  accuracy: 0.8235294117647058\n",
      "sklearn fold  0  accuracy:  0.8823529411764706\n",
      "My DT Fold 0  accuracy: 1.0\n",
      "sklearn fold  1  accuracy:  0.7647058823529411\n",
      "My DT Fold 1  accuracy: 0.7647058823529411\n",
      "sklearn fold  2  accuracy:  0.8823529411764706\n",
      "My DT Fold 2  accuracy: 0.9411764705882353\n",
      "sklearn fold  3  accuracy:  0.9411764705882353\n",
      "My DT Fold 3  accuracy: 0.8823529411764706\n",
      "sklearn fold  4  accuracy:  0.9411764705882353\n",
      "My DT Fold 4  accuracy: 0.8823529411764706\n",
      "sklearn fold  5  accuracy:  0.9411764705882353\n",
      "My DT Fold 5  accuracy: 0.8823529411764706\n",
      "sklearn fold  6  accuracy:  1.0\n",
      "My DT Fold 6  accuracy: 0.8823529411764706\n",
      "sklearn fold  7  accuracy:  0.8823529411764706\n",
      "My DT Fold 7  accuracy: 0.8235294117647058\n",
      "sklearn fold  8  accuracy:  1.0\n",
      "My DT Fold 8  accuracy: 1.0\n",
      "sklearn fold  9  accuracy:  0.8823529411764706\n",
      "My DT Fold 9  accuracy: 0.7058823529411765\n",
      "sklearn fold  0  accuracy:  0.9411764705882353\n",
      "My DT Fold 0  accuracy: 0.7647058823529411\n",
      "sklearn fold  1  accuracy:  0.9411764705882353\n",
      "My DT Fold 1  accuracy: 0.8823529411764706\n",
      "sklearn fold  2  accuracy:  0.8235294117647058\n",
      "My DT Fold 2  accuracy: 0.8235294117647058\n",
      "sklearn fold  3  accuracy:  0.9411764705882353\n",
      "My DT Fold 3  accuracy: 0.9411764705882353\n",
      "sklearn fold  4  accuracy:  1.0\n",
      "My DT Fold 4  accuracy: 0.9411764705882353\n",
      "sklearn fold  5  accuracy:  1.0\n",
      "My DT Fold 5  accuracy: 0.7647058823529411\n",
      "sklearn fold  6  accuracy:  0.9411764705882353\n",
      "My DT Fold 6  accuracy: 0.8823529411764706\n",
      "sklearn fold  7  accuracy:  0.8823529411764706\n",
      "My DT Fold 7  accuracy: 0.8823529411764706\n",
      "sklearn fold  8  accuracy:  0.9411764705882353\n",
      "My DT Fold 8  accuracy: 0.9411764705882353\n",
      "sklearn fold  9  accuracy:  0.9411764705882353\n",
      "My DT Fold 9  accuracy: 0.8823529411764706\n",
      "sklearn fold  0  accuracy:  0.8823529411764706\n",
      "My DT Fold 0  accuracy: 0.8823529411764706\n",
      "sklearn fold  1  accuracy:  0.9411764705882353\n",
      "My DT Fold 1  accuracy: 0.9411764705882353\n",
      "sklearn fold  2  accuracy:  0.7647058823529411\n",
      "My DT Fold 2  accuracy: 0.8235294117647058\n",
      "sklearn fold  3  accuracy:  0.9411764705882353\n",
      "My DT Fold 3  accuracy: 1.0\n",
      "sklearn fold  4  accuracy:  0.9411764705882353\n",
      "My DT Fold 4  accuracy: 0.8823529411764706\n",
      "sklearn fold  5  accuracy:  0.8235294117647058\n",
      "My DT Fold 5  accuracy: 0.8235294117647058\n",
      "sklearn fold  6  accuracy:  0.9411764705882353\n",
      "My DT Fold 6  accuracy: 0.8235294117647058\n",
      "sklearn fold  7  accuracy:  0.8823529411764706\n",
      "My DT Fold 7  accuracy: 0.8823529411764706\n",
      "sklearn fold  8  accuracy:  0.9411764705882353\n",
      "My DT Fold 8  accuracy: 0.8235294117647058\n",
      "sklearn fold  9  accuracy:  1.0\n",
      "My DT Fold 9  accuracy: 0.9411764705882353\n",
      "sklearn fold  0  accuracy:  0.8823529411764706\n",
      "My DT Fold 0  accuracy: 0.8235294117647058\n",
      "sklearn fold  1  accuracy:  0.9411764705882353\n",
      "My DT Fold 1  accuracy: 1.0\n",
      "sklearn fold  2  accuracy:  0.9411764705882353\n",
      "My DT Fold 2  accuracy: 0.8823529411764706\n",
      "sklearn fold  3  accuracy:  0.9411764705882353\n",
      "My DT Fold 3  accuracy: 0.9411764705882353\n",
      "sklearn fold  4  accuracy:  1.0\n",
      "My DT Fold 4  accuracy: 0.8823529411764706\n",
      "sklearn fold  5  accuracy:  0.8823529411764706\n",
      "My DT Fold 5  accuracy: 0.7058823529411765\n",
      "sklearn fold  6  accuracy:  0.8235294117647058\n",
      "My DT Fold 6  accuracy: 0.8235294117647058\n",
      "sklearn fold  7  accuracy:  0.8235294117647058\n",
      "My DT Fold 7  accuracy: 0.8823529411764706\n",
      "sklearn fold  8  accuracy:  0.9411764705882353\n",
      "My DT Fold 8  accuracy: 0.8823529411764706\n",
      "sklearn fold  9  accuracy:  0.8823529411764706\n",
      "My DT Fold 9  accuracy: 0.9411764705882353\n",
      "sklearn fold  0  accuracy:  0.8235294117647058\n",
      "My DT Fold 0  accuracy: 0.8823529411764706\n",
      "sklearn fold  1  accuracy:  0.8823529411764706\n",
      "My DT Fold 1  accuracy: 0.8823529411764706\n",
      "sklearn fold  2  accuracy:  0.7647058823529411\n",
      "My DT Fold 2  accuracy: 0.9411764705882353\n",
      "sklearn fold  3  accuracy:  1.0\n",
      "My DT Fold 3  accuracy: 1.0\n",
      "sklearn fold  4  accuracy:  1.0\n",
      "My DT Fold 4  accuracy: 1.0\n",
      "sklearn fold  5  accuracy:  0.7647058823529411\n",
      "My DT Fold 5  accuracy: 0.9411764705882353\n",
      "sklearn fold  6  accuracy:  0.8235294117647058\n",
      "My DT Fold 6  accuracy: 1.0\n",
      "sklearn fold  7  accuracy:  0.8823529411764706\n",
      "My DT Fold 7  accuracy: 0.8823529411764706\n",
      "sklearn fold  8  accuracy:  0.8823529411764706\n",
      "My DT Fold 8  accuracy: 0.9411764705882353\n",
      "sklearn fold  9  accuracy:  0.9411764705882353\n",
      "My DT Fold 9  accuracy: 0.8235294117647058\n",
      "sklearn fold  0  accuracy:  0.8823529411764706\n",
      "My DT Fold 0  accuracy: 0.9411764705882353\n",
      "sklearn fold  1  accuracy:  0.8823529411764706\n",
      "My DT Fold 1  accuracy: 0.8823529411764706\n",
      "sklearn fold  2  accuracy:  0.7647058823529411\n",
      "My DT Fold 2  accuracy: 0.7647058823529411\n",
      "sklearn fold  3  accuracy:  0.9411764705882353\n",
      "My DT Fold 3  accuracy: 1.0\n",
      "sklearn fold  4  accuracy:  0.9411764705882353\n",
      "My DT Fold 4  accuracy: 0.8823529411764706\n",
      "sklearn fold  5  accuracy:  0.9411764705882353\n",
      "My DT Fold 5  accuracy: 0.8235294117647058\n",
      "sklearn fold  6  accuracy:  0.9411764705882353\n",
      "My DT Fold 6  accuracy: 1.0\n",
      "sklearn fold  7  accuracy:  0.9411764705882353\n",
      "My DT Fold 7  accuracy: 0.9411764705882353\n",
      "sklearn fold  8  accuracy:  0.9411764705882353\n",
      "My DT Fold 8  accuracy: 1.0\n",
      "sklearn fold  9  accuracy:  0.8823529411764706\n",
      "My DT Fold 9  accuracy: 0.8235294117647058\n",
      "sklearn Decision Tree accuracy score test  0 :  0.9235294117647059\n",
      "sklearn Decision Tree accuracy score test  1 :  0.9176470588235294\n",
      "sklearn Decision Tree accuracy score test  2 :  0.9176470588235295\n",
      "sklearn Decision Tree accuracy score test  3 :  0.9176470588235295\n",
      "sklearn Decision Tree accuracy score test  4 :  0.9117647058823529\n",
      "sklearn Decision Tree accuracy score test  5 :  0.9352941176470588\n",
      "sklearn Decision Tree accuracy score test  6 :  0.9058823529411765\n",
      "sklearn Decision Tree accuracy score test  7 :  0.9058823529411765\n",
      "sklearn Decision Tree accuracy score test  8 :  0.8764705882352942\n",
      "sklearn Decision Tree accuracy score test  9 :  0.9058823529411765\n",
      "****************************************************************************************************\n",
      "My Decision Tree accuracy score test  0 :  0.8941176470588236\n",
      "My Decision Tree accuracy score test  1 :  0.9294117647058824\n",
      "My Decision Tree accuracy score test  2 :  0.876470588235294\n",
      "My Decision Tree accuracy score test  3 :  0.9\n",
      "My Decision Tree accuracy score test  4 :  0.876470588235294\n",
      "My Decision Tree accuracy score test  5 :  0.8705882352941178\n",
      "My Decision Tree accuracy score test  6 :  0.8823529411764705\n",
      "My Decision Tree accuracy score test  7 :  0.8764705882352942\n",
      "My Decision Tree accuracy score test  8 :  0.9294117647058824\n",
      "My Decision Tree accuracy score test  9 :  0.9058823529411765\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "#Wine dataset\n",
    "print(\"Wine Dataset\")\n",
    "# initialize sklearn test scores list\n",
    "sklearn_test_scores = []\n",
    "# initialize my decision tree test scores\n",
    "my_dt_test_scores = []\n",
    "# 10 different tests on a 10 fold cross validation\n",
    "for times in range(10):\n",
    "    # initialize current accuracy lists shuffle dataset\n",
    "    sklearn_accuracy = []\n",
    "    my_accuracy = []\n",
    "    dataset2 = dataset2.sample(frac=1)\n",
    "    dataset2 = dataset2.sample(frac=1)\n",
    "    # split into 10 parts\n",
    "    parts2 = cross_validator(cv, test_size2, dataset2)\n",
    "    for fold in range(cv):\n",
    "        # initialize scikit learn decision tree classifier\n",
    "        scikitDT = Scikit_DTC()\n",
    "        # iniitalize the decision tree classifie\n",
    "        dt = DecisionTreeClassifier()\n",
    "        # test part for the set on particular fold\n",
    "        test = parts2[fold]\n",
    "        # combine all parts except for the test part\n",
    "        train_list  = [parts2[train_part] for train_part in range(cv) if fold != train_part]\n",
    "        # train set\n",
    "        train = pd.concat(train_list)\n",
    "        # split into X for the features and y for target class for both test and train sets\n",
    "        X_test = test.iloc[:, :-1].astype(float).reset_index(drop=True)\n",
    "        y_test = test.iloc[:, -1].reset_index(drop=True)\n",
    "        # both X train and X test sets have the datatype set to float so that numpy can operate\n",
    "        X_train = train.iloc[:, :-1].astype(float).reset_index(drop=True)\n",
    "        y_train = train.iloc[:, -1].reset_index(drop=True)\n",
    "        # scikit learn fits the model then predics returning a list of predictions for y\n",
    "        # adds the reults to scikit-learn list\n",
    "        scikitDT.fit(X_train, y_train)\n",
    "        sky_pred = scikitDT.predict(X_test)\n",
    "        sklearn_acc = accuracy_score(sky_pred, y_test)\n",
    "        print(\"sklearn fold \", fold, \" accuracy: \", sklearn_acc)\n",
    "        sklearn_accuracy.append(sklearn_acc)\n",
    "        # the decision tree classifier train\n",
    "        dt.fit(X_train, y_train, datatype2)\n",
    "        # the decision tree classifier predictions\n",
    "        y_pred = dt.predict(X_test)\n",
    "        # append score to mydt scores list\n",
    "        my_acc = accuracy_score(y_pred, y_test)\n",
    "        my_accuracy.append(my_acc)\n",
    "        print(\"My DT Fold\", fold, \" accuracy:\", my_acc)\n",
    "    # calculate average of 10 for cross validation\n",
    "    sklearn_avg_accuracy = np.array(sklearn_accuracy).mean(axis=0)\n",
    "    # add to overall scores list\n",
    "    sklearn_test_scores.append(sklearn_avg_accuracy)\n",
    "    # calculate average and store in overall scores list\n",
    "    my_avg_accuracy = np.array(my_accuracy).mean(axis=0)\n",
    "    my_dt_test_scores.append(my_avg_accuracy)\n",
    "\n",
    "for i in range(len(sklearn_test_scores)):\n",
    "    print(\"sklearn Decision Tree accuracy score test \", i, \": \", sklearn_test_scores[i])\n",
    "print(\"*\" * 100)\n",
    "\n",
    "for i in range(len(my_dt_test_scores)):\n",
    "    print(\"My Decision Tree accuracy score test \", i, \": \", my_dt_test_scores[i])\n",
    "print(\"*\" * 100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BREAST-CANCER DATASET 10 X 10-FOLD CROSS VALIADATION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breast Cancer Dataset\n",
      "sklearn fold  0  accuracy:  0.6071428571428571\n",
      "My DT Fold 0  accuracy: 0.5\n",
      "sklearn fold  1  accuracy:  0.6785714285714286\n",
      "My DT Fold 1  accuracy: 0.7857142857142857\n",
      "sklearn fold  2  accuracy:  0.6785714285714286\n",
      "My DT Fold 2  accuracy: 0.6428571428571429\n",
      "sklearn fold  3  accuracy:  0.6071428571428571\n",
      "My DT Fold 3  accuracy: 0.7857142857142857\n",
      "sklearn fold  4  accuracy:  0.6071428571428571\n",
      "My DT Fold 4  accuracy: 0.6428571428571429\n",
      "sklearn fold  5  accuracy:  0.6071428571428571\n",
      "My DT Fold 5  accuracy: 0.6785714285714286\n",
      "sklearn fold  6  accuracy:  0.6428571428571429\n",
      "My DT Fold 6  accuracy: 0.7857142857142857\n",
      "sklearn fold  7  accuracy:  0.6071428571428571\n",
      "My DT Fold 7  accuracy: 0.6428571428571429\n",
      "sklearn fold  8  accuracy:  0.6428571428571429\n",
      "My DT Fold 8  accuracy: 0.75\n",
      "sklearn fold  9  accuracy:  0.7857142857142857\n",
      "My DT Fold 9  accuracy: 0.7857142857142857\n",
      "sklearn fold  0  accuracy:  0.75\n",
      "My DT Fold 0  accuracy: 0.7142857142857143\n",
      "sklearn fold  1  accuracy:  0.5\n",
      "My DT Fold 1  accuracy: 0.6071428571428571\n",
      "sklearn fold  2  accuracy:  0.5357142857142857\n",
      "My DT Fold 2  accuracy: 0.6428571428571429\n",
      "sklearn fold  3  accuracy:  0.6428571428571429\n",
      "My DT Fold 3  accuracy: 0.6071428571428571\n",
      "sklearn fold  4  accuracy:  0.6785714285714286\n",
      "My DT Fold 4  accuracy: 0.7142857142857143\n",
      "sklearn fold  5  accuracy:  0.5357142857142857\n",
      "My DT Fold 5  accuracy: 0.75\n",
      "sklearn fold  6  accuracy:  0.5357142857142857\n",
      "My DT Fold 6  accuracy: 0.5357142857142857\n",
      "sklearn fold  7  accuracy:  0.5714285714285714\n",
      "My DT Fold 7  accuracy: 0.5714285714285714\n",
      "sklearn fold  8  accuracy:  0.5714285714285714\n",
      "My DT Fold 8  accuracy: 0.6785714285714286\n",
      "sklearn fold  9  accuracy:  0.75\n",
      "My DT Fold 9  accuracy: 0.7142857142857143\n",
      "sklearn fold  0  accuracy:  0.6071428571428571\n",
      "My DT Fold 0  accuracy: 0.7142857142857143\n",
      "sklearn fold  1  accuracy:  0.6071428571428571\n",
      "My DT Fold 1  accuracy: 0.8214285714285714\n",
      "sklearn fold  2  accuracy:  0.5357142857142857\n",
      "My DT Fold 2  accuracy: 0.6071428571428571\n",
      "sklearn fold  3  accuracy:  0.6785714285714286\n",
      "My DT Fold 3  accuracy: 0.5357142857142857\n",
      "sklearn fold  4  accuracy:  0.5357142857142857\n",
      "My DT Fold 4  accuracy: 0.5714285714285714\n",
      "sklearn fold  5  accuracy:  0.42857142857142855\n",
      "My DT Fold 5  accuracy: 0.6428571428571429\n",
      "sklearn fold  6  accuracy:  0.6071428571428571\n",
      "My DT Fold 6  accuracy: 0.75\n",
      "sklearn fold  7  accuracy:  0.6785714285714286\n",
      "My DT Fold 7  accuracy: 0.6071428571428571\n",
      "sklearn fold  8  accuracy:  0.6071428571428571\n",
      "My DT Fold 8  accuracy: 0.6428571428571429\n",
      "sklearn fold  9  accuracy:  0.7142857142857143\n",
      "My DT Fold 9  accuracy: 0.7142857142857143\n",
      "sklearn fold  0  accuracy:  0.5714285714285714\n",
      "My DT Fold 0  accuracy: 0.6785714285714286\n",
      "sklearn fold  1  accuracy:  0.6785714285714286\n",
      "My DT Fold 1  accuracy: 0.7142857142857143\n",
      "sklearn fold  2  accuracy:  0.42857142857142855\n",
      "My DT Fold 2  accuracy: 0.6428571428571429\n",
      "sklearn fold  3  accuracy:  0.6428571428571429\n",
      "My DT Fold 3  accuracy: 0.6785714285714286\n",
      "sklearn fold  4  accuracy:  0.6428571428571429\n",
      "My DT Fold 4  accuracy: 0.6071428571428571\n",
      "sklearn fold  5  accuracy:  0.6785714285714286\n",
      "My DT Fold 5  accuracy: 0.6785714285714286\n",
      "sklearn fold  6  accuracy:  0.6428571428571429\n",
      "My DT Fold 6  accuracy: 0.6785714285714286\n",
      "sklearn fold  7  accuracy:  0.6071428571428571\n",
      "My DT Fold 7  accuracy: 0.7857142857142857\n",
      "sklearn fold  8  accuracy:  0.6428571428571429\n",
      "My DT Fold 8  accuracy: 0.6785714285714286\n",
      "sklearn fold  9  accuracy:  0.4642857142857143\n",
      "My DT Fold 9  accuracy: 0.6071428571428571\n",
      "sklearn fold  0  accuracy:  0.6785714285714286\n",
      "My DT Fold 0  accuracy: 0.6428571428571429\n",
      "sklearn fold  1  accuracy:  0.6428571428571429\n",
      "My DT Fold 1  accuracy: 0.5714285714285714\n",
      "sklearn fold  2  accuracy:  0.6785714285714286\n",
      "My DT Fold 2  accuracy: 0.6785714285714286\n",
      "sklearn fold  3  accuracy:  0.6428571428571429\n",
      "My DT Fold 3  accuracy: 0.7857142857142857\n",
      "sklearn fold  4  accuracy:  0.6785714285714286\n",
      "My DT Fold 4  accuracy: 0.8571428571428571\n",
      "sklearn fold  5  accuracy:  0.6785714285714286\n",
      "My DT Fold 5  accuracy: 0.6785714285714286\n",
      "sklearn fold  6  accuracy:  0.6071428571428571\n",
      "My DT Fold 6  accuracy: 0.6071428571428571\n",
      "sklearn fold  7  accuracy:  0.7142857142857143\n",
      "My DT Fold 7  accuracy: 0.7142857142857143\n",
      "sklearn fold  8  accuracy:  0.7142857142857143\n",
      "My DT Fold 8  accuracy: 0.6428571428571429\n",
      "sklearn fold  9  accuracy:  0.6785714285714286\n",
      "My DT Fold 9  accuracy: 0.7857142857142857\n",
      "sklearn fold  0  accuracy:  0.5357142857142857\n",
      "My DT Fold 0  accuracy: 0.6428571428571429\n",
      "sklearn fold  1  accuracy:  0.8214285714285714\n",
      "My DT Fold 1  accuracy: 0.7857142857142857\n",
      "sklearn fold  2  accuracy:  0.6071428571428571\n",
      "My DT Fold 2  accuracy: 0.6785714285714286\n",
      "sklearn fold  3  accuracy:  0.8214285714285714\n",
      "My DT Fold 3  accuracy: 0.6428571428571429\n",
      "sklearn fold  4  accuracy:  0.5\n",
      "My DT Fold 4  accuracy: 0.5714285714285714\n",
      "sklearn fold  5  accuracy:  0.5357142857142857\n",
      "My DT Fold 5  accuracy: 0.5\n",
      "sklearn fold  6  accuracy:  0.6428571428571429\n",
      "My DT Fold 6  accuracy: 0.6785714285714286\n",
      "sklearn fold  7  accuracy:  0.75\n",
      "My DT Fold 7  accuracy: 0.6071428571428571\n",
      "sklearn fold  8  accuracy:  0.5357142857142857\n",
      "My DT Fold 8  accuracy: 0.6071428571428571\n",
      "sklearn fold  9  accuracy:  0.6071428571428571\n",
      "My DT Fold 9  accuracy: 0.6071428571428571\n",
      "sklearn fold  0  accuracy:  0.5\n",
      "My DT Fold 0  accuracy: 0.7142857142857143\n",
      "sklearn fold  1  accuracy:  0.5714285714285714\n",
      "My DT Fold 1  accuracy: 0.75\n",
      "sklearn fold  2  accuracy:  0.75\n",
      "My DT Fold 2  accuracy: 0.6785714285714286\n",
      "sklearn fold  3  accuracy:  0.6785714285714286\n",
      "My DT Fold 3  accuracy: 0.8214285714285714\n",
      "sklearn fold  4  accuracy:  0.6428571428571429\n",
      "My DT Fold 4  accuracy: 0.5714285714285714\n",
      "sklearn fold  5  accuracy:  0.5357142857142857\n",
      "My DT Fold 5  accuracy: 0.6071428571428571\n",
      "sklearn fold  6  accuracy:  0.7142857142857143\n",
      "My DT Fold 6  accuracy: 0.7142857142857143\n",
      "sklearn fold  7  accuracy:  0.6428571428571429\n",
      "My DT Fold 7  accuracy: 0.6071428571428571\n",
      "sklearn fold  8  accuracy:  0.6071428571428571\n",
      "My DT Fold 8  accuracy: 0.5714285714285714\n",
      "sklearn fold  9  accuracy:  0.5714285714285714\n",
      "My DT Fold 9  accuracy: 0.5\n",
      "sklearn fold  0  accuracy:  0.5714285714285714\n",
      "My DT Fold 0  accuracy: 0.4642857142857143\n",
      "sklearn fold  1  accuracy:  0.7142857142857143\n",
      "My DT Fold 1  accuracy: 0.6785714285714286\n",
      "sklearn fold  2  accuracy:  0.5714285714285714\n",
      "My DT Fold 2  accuracy: 0.7142857142857143\n",
      "sklearn fold  3  accuracy:  0.7142857142857143\n",
      "My DT Fold 3  accuracy: 0.6785714285714286\n",
      "sklearn fold  4  accuracy:  0.6428571428571429\n",
      "My DT Fold 4  accuracy: 0.6785714285714286\n",
      "sklearn fold  5  accuracy:  0.6071428571428571\n",
      "My DT Fold 5  accuracy: 0.4642857142857143\n",
      "sklearn fold  6  accuracy:  0.5\n",
      "My DT Fold 6  accuracy: 0.6071428571428571\n",
      "sklearn fold  7  accuracy:  0.7142857142857143\n",
      "My DT Fold 7  accuracy: 0.6071428571428571\n",
      "sklearn fold  8  accuracy:  0.7142857142857143\n",
      "My DT Fold 8  accuracy: 0.6428571428571429\n",
      "sklearn fold  9  accuracy:  0.6071428571428571\n",
      "My DT Fold 9  accuracy: 0.5714285714285714\n",
      "sklearn fold  0  accuracy:  0.5714285714285714\n",
      "My DT Fold 0  accuracy: 0.6785714285714286\n",
      "sklearn fold  1  accuracy:  0.8214285714285714\n",
      "My DT Fold 1  accuracy: 0.6428571428571429\n",
      "sklearn fold  2  accuracy:  0.6428571428571429\n",
      "My DT Fold 2  accuracy: 0.6785714285714286\n",
      "sklearn fold  3  accuracy:  0.5714285714285714\n",
      "My DT Fold 3  accuracy: 0.6071428571428571\n",
      "sklearn fold  4  accuracy:  0.6785714285714286\n",
      "My DT Fold 4  accuracy: 0.6785714285714286\n",
      "sklearn fold  5  accuracy:  0.6785714285714286\n",
      "My DT Fold 5  accuracy: 0.75\n",
      "sklearn fold  6  accuracy:  0.6071428571428571\n",
      "My DT Fold 6  accuracy: 0.8214285714285714\n",
      "sklearn fold  7  accuracy:  0.6785714285714286\n",
      "My DT Fold 7  accuracy: 0.6428571428571429\n",
      "sklearn fold  8  accuracy:  0.7142857142857143\n",
      "My DT Fold 8  accuracy: 0.6785714285714286\n",
      "sklearn fold  9  accuracy:  0.6428571428571429\n",
      "My DT Fold 9  accuracy: 0.5714285714285714\n",
      "sklearn fold  0  accuracy:  0.5357142857142857\n",
      "My DT Fold 0  accuracy: 0.6428571428571429\n",
      "sklearn fold  1  accuracy:  0.5714285714285714\n",
      "My DT Fold 1  accuracy: 0.6785714285714286\n",
      "sklearn fold  2  accuracy:  0.75\n",
      "My DT Fold 2  accuracy: 0.8214285714285714\n",
      "sklearn fold  3  accuracy:  0.6071428571428571\n",
      "My DT Fold 3  accuracy: 0.7142857142857143\n",
      "sklearn fold  4  accuracy:  0.7142857142857143\n",
      "My DT Fold 4  accuracy: 0.75\n",
      "sklearn fold  5  accuracy:  0.6428571428571429\n",
      "My DT Fold 5  accuracy: 0.5357142857142857\n",
      "sklearn fold  6  accuracy:  0.7142857142857143\n",
      "My DT Fold 6  accuracy: 0.7857142857142857\n",
      "sklearn fold  7  accuracy:  0.6071428571428571\n",
      "My DT Fold 7  accuracy: 0.6428571428571429\n",
      "sklearn fold  8  accuracy:  0.6071428571428571\n",
      "My DT Fold 8  accuracy: 0.5\n",
      "sklearn fold  9  accuracy:  0.5357142857142857\n",
      "My DT Fold 9  accuracy: 0.6071428571428571\n",
      "sklearn Decision Tree accuracy score test  0 :  0.6464285714285715\n",
      "sklearn Decision Tree accuracy score test  1 :  0.6071428571428571\n",
      "sklearn Decision Tree accuracy score test  2 :  0.6\n",
      "sklearn Decision Tree accuracy score test  3 :  0.6\n",
      "sklearn Decision Tree accuracy score test  4 :  0.6714285714285715\n",
      "sklearn Decision Tree accuracy score test  5 :  0.6357142857142856\n",
      "sklearn Decision Tree accuracy score test  6 :  0.6214285714285713\n",
      "sklearn Decision Tree accuracy score test  7 :  0.6357142857142857\n",
      "sklearn Decision Tree accuracy score test  8 :  0.6607142857142858\n",
      "sklearn Decision Tree accuracy score test  9 :  0.6285714285714286\n",
      "****************************************************************************************************\n",
      "My Decision Tree accuracy score test  0 :  0.7\n",
      "My Decision Tree accuracy score test  1 :  0.6535714285714286\n",
      "My Decision Tree accuracy score test  2 :  0.6607142857142858\n",
      "My Decision Tree accuracy score test  3 :  0.675\n",
      "My Decision Tree accuracy score test  4 :  0.6964285714285714\n",
      "My Decision Tree accuracy score test  5 :  0.632142857142857\n",
      "My Decision Tree accuracy score test  6 :  0.6535714285714286\n",
      "My Decision Tree accuracy score test  7 :  0.6107142857142857\n",
      "My Decision Tree accuracy score test  8 :  0.675\n",
      "My Decision Tree accuracy score test  9 :  0.6678571428571428\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "#Breast-Cancer dataset\n",
    "print(\"Breast Cancer Dataset\")\n",
    "sklearn_test_scores = []\n",
    "my_dt_test_scores = []\n",
    "for times in range(10):\n",
    "    sklearn_accuracy = []\n",
    "    my_accuracy = []\n",
    "    dataset3 = dataset3.sample(frac=1)\n",
    "    dataset3 = dataset3.sample(frac=1)\n",
    "    parts3 = cross_validator(cv, test_size3, dataset3)\n",
    "    for fold in range(cv):\n",
    "        scikitDT = Scikit_DTC()\n",
    "        dt = DecisionTreeClassifier()\n",
    "        # test part for the set on particular fold\n",
    "        test = parts3[fold]\n",
    "        # combine all parts except for the test part\n",
    "        train_list  = [parts3[train_part] for train_part in range(cv) if fold != train_part]\n",
    "        # train set\n",
    "        train = pd.concat(train_list)\n",
    "        # split into X for the features and y for target class for both test and train sets\n",
    "        X_test = test.iloc[:, :-1].astype(float).reset_index(drop=True)\n",
    "        # print(X_test)\n",
    "        y_test = test.iloc[:, -1].reset_index(drop=True)\n",
    "        # both X train and X test sets have the datatype set to float so that numpy can operate\n",
    "        X_train = train.iloc[:, :-1].astype(float).reset_index(drop=True)\n",
    "        # print(X_train)\n",
    "        y_train = train.iloc[:, -1].reset_index(drop=True)\n",
    "        scikitDT.fit(X_train, y_train)\n",
    "        sky_pred = scikitDT.predict(X_test)\n",
    "        sklearn_acc = accuracy_score(sky_pred, y_test)\n",
    "        print(\"sklearn fold \", fold, \" accuracy: \", sklearn_acc)\n",
    "        sklearn_accuracy.append(sklearn_acc)\n",
    "        # print(datatype3)\n",
    "        dt.fit(X_train, y_train, datatype3)\n",
    "        y_pred = dt.predict(X_test)\n",
    "        my_acc = accuracy_score(y_pred, y_test)\n",
    "        my_accuracy.append(my_acc)\n",
    "        print(\"My DT Fold\", fold, \" accuracy:\", my_acc)\n",
    "    sklearn_avg_accuracy = np.array(sklearn_accuracy).mean(axis=0)\n",
    "    sklearn_test_scores.append(sklearn_avg_accuracy)\n",
    "    my_avg_accuracy = np.array(my_accuracy).mean(axis=0)\n",
    "    my_dt_test_scores.append(my_avg_accuracy)\n",
    "\n",
    "for i in range(len(sklearn_test_scores)):\n",
    "    print(\"sklearn Decision Tree accuracy score test \", i, \": \", sklearn_test_scores[i])\n",
    "print(\"*\" * 100)\n",
    "\n",
    "for i in range(len(my_dt_test_scores)):\n",
    "    print(\"My Decision Tree accuracy score test \", i, \": \", my_dt_test_scores[i])\n",
    "print(\"*\" * 100)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
