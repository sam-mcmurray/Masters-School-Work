{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Converting Models to TFLite and Running Inference\n",
    "\n",
    "https://www.tensorflow.org/lite/models/convert\n",
    "\n",
    "In this exercise you will:\n",
    "- Learn how to convert a Tensorflow/Keras model to TensorFlow Lite (TFLite).\n",
    "- Learn how to use the TFLite Interpreter to run inference on a TFLite model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, let's import the python modules we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Lite Converter\n",
    "\n",
    "<img width=\"400\" height=\"500\" src=\"../notebook_images/tflite_convert.png\" style=\"padding: 0px; float: right;\">\n",
    "\n",
    "The TensorFlow Lite converter takes a TensorFlow model and generates a TensorFlow Lite model (an optimized [FlatBuffer](https://google.github.io/flatbuffers) format identified by the `.tflite` file extension). You have the following two options for using the converter:\n",
    "\n",
    "1. Python API (**recommended**): This makes it easier to convert models as part of the model development pipeline, apply optimizations, add metadata and has many more features.\n",
    "2. Command line: This only supports basic model conversion.\n",
    "\n",
    "### Python API\n",
    "\n",
    "Helper code: To identify the installed TensorFlow version, run `print(tf.__version__)` and to learn more about the TensorFlow Lite converter API, run `print(help(tf.lite.TFLiteConverter))`.\n",
    "\n",
    "If you've installed [TensorFlow 2.x](https://www.tensorflow.org/install/pip#tensorflow-2-packages-are-available), you have the following two options (if you've installed [TensorFlow 1.x](https://www.tensorflow.org/install/pip#older-versions-of-tensorflow), refer to [Github](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/python_api.md)):\n",
    "\n",
    "- Convert a TensorFlow 2.x model using [tf.lite.TFLiteConverter](https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter). A TensorFlow 2.x model is stored using the SavedModel format and is generated either using the high-level `tf.keras.*` APIs (a Keras model) or the low-level `tf.*` APIs (from which you generate concrete functions). As a result, you have the following three options (examples are in the next few sections):\n",
    "  - [tf.lite.TFLiteConverter.from_saved_model()](https://www.tensorflow.org/lite/api_docs/python/tf/lite/TFLiteConverter#from_saved_model) (**recommended**): Converts a [SavedModel](https://www.tensorflow.org/guide/saved_model).\n",
    "  - [tf.lite.TFLiteConverter.from_keras_model()](https://www.tensorflow.org/lite/api_docs/python/tf/lite/TFLiteConverter#from_keras_model): Converts a [Keras](https://www.tensorflow.org/guide/keras/sequential_model) model.\n",
    "  - [tf.lite.TFLiteConverter.from_concrete_functions()](https://www.tensorflow.org/lite/api_docs/python/tf/lite/TFLiteConverter#from_concrete_functions): Converts [concrete functions](https://www.tensorflow.org/guide/intro_to_graphs).\n",
    "- Convert a TensorFlow 1.x model using [tf.compat.v1.lite.TFLiteConverter](https://www.tensorflow.org/api_docs/python/tf/compat/v1/lite/TFLiteConverter) (examples are on [Github](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/python_api.md)):\n",
    "\n",
    "  - [tf.compat.v1.lite.TFLiteConverter.from_saved_model()](https://www.tensorflow.org/api_docs/python/tf/compat/v1/lite/TFLiteConverter#from_saved_model): Converts a [SavedModel](https://www.tensorflow.org/guide/saved_model).\n",
    "  - [tf.compat.v1.lite.TFLiteConverter.from_keras_model_file()](https://www.tensorflow.org/api_docs/python/tf/compat/v1/lite/TFLiteConverter#from_keras_model_file): Converts a [Keras](https://www.tensorflow.org/guide/keras/sequential_model) model.\n",
    "  - [tf.compat.v1.lite.TFLiteConverter.from_session()](https://www.tensorflow.org/api_docs/python/tf/compat/v1/lite/TFLiteConverter#from_session): Converts a GraphDef from a session.\n",
    "  - [tf.compat.v1.lite.TFLiteConverter.from_frozen_graph()](https://www.tensorflow.org/api_docs/python/tf/compat/v1/lite/TFLiteConverter#from_frozen_graph): Converts a Frozen GraphDef from a file. If you have checkpoints, then first convert it to a Frozen GraphDef file and then use this API as shown [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/python_api.md#checkpoints).\n",
    "\n",
    "  All examples below use TensorFlow 2.x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert a SavedModel (**recommended**)\n",
    "\n",
    "The following example shows how to convert a [SavedModel](https://www.tensorflow.org/guide/saved_model) into a TensorFlow Lite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model (let's use the saved model from the previous exercise)\n",
    "import_path = os.path.join(os.getcwd(),'saved_models','model_1')\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(import_path) # path to the SavedModel directory\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Let's create the folder \"saved_tflite_models\" and store the TFLite models there\n",
    "os.makedirs(os.path.join(os.getcwd(),'saved_tflite_models'), exist_ok = True)\n",
    "\n",
    "# Save the model\n",
    "tflite_model_path = os.path.join(os.getcwd(),'saved_tflite_models','model_1.tflite')\n",
    "with open(tflite_model_path, 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert a Keras model \n",
    "\n",
    "The following example shows how to convert a [Keras](https://www.tensorflow.org/guide/keras/sequential_model) model into a TensorFlow Lite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.2211\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.9297\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.6536\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.3946\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.1499\n",
      "INFO:tensorflow:Assets written to: c:\\Users\\gabpat\\projects\\TEDS22\\Workshop3\\saved_models\\model_2\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\gabpat\\AppData\\Local\\Temp\\tmpma3gvqld\\assets\n"
     ]
    }
   ],
   "source": [
    "# Create a model using high-level tf.keras.* APIs\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units=1, input_shape=[1]),\n",
    "    tf.keras.layers.Dense(units=16, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(x=[-1, 0, 1], y=[-3, -1, 1], epochs=5)\n",
    "\n",
    "# Save the model as a SavedModel (this step isn't really necessary)\n",
    "tfcore_model2_path = os.path.join(os.getcwd(),'saved_models','model_2')\n",
    "tf.saved_model.save(model, tfcore_model2_path)\n",
    "\n",
    "# Convert the model (note: we are converting the model directly from the \"model\" variable)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Let's save the model as \"model_2.tflite\"\n",
    "tflite_model2_path = os.path.join(os.getcwd(),'saved_tflite_models','model_2.tflite')\n",
    "with open(tflite_model2_path, 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert concrete functions \n",
    "\n",
    "The following example shows how to convert [concrete functions](https://www.tensorflow.org/guide/intro_to_graphs) into a TensorFlow Lite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(25.0, shape=(), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: c:\\Users\\gabpat\\projects\\TEDS22\\Workshop3\\saved_models\\model_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\gabpat\\projects\\TEDS22\\Workshop3\\saved_models\\model_3\\assets\n",
      "WARNING:absl:Please consider providing the trackable_obj argument in the from_concrete_functions. Providing without the trackable_obj argument is deprecated and it will use the deprecated conversion path.\n"
     ]
    }
   ],
   "source": [
    "# Create a model using low-level tf.* APIs\n",
    "class Squared(tf.Module):\n",
    "  @tf.function\n",
    "  def __call__(self, x):\n",
    "    return tf.square(x)\n",
    "model = Squared()\n",
    "\n",
    "# Let's call the function (this step isn't really necessary)\n",
    "input_data = tf.constant(5.0)\n",
    "answer = model(input_data)\n",
    "print(answer)\n",
    "\n",
    "# Save the model as a SavedModel (this step isn't really necessary)\n",
    "tfcore_model3_path = os.path.join(os.getcwd(),'saved_models','model_3')\n",
    "tf.saved_model.save(model, tfcore_model3_path)\n",
    "\n",
    "# Get the \"concrete function\"\n",
    "concrete_func = model.__call__.get_concrete_function(input_data)\n",
    "\n",
    "# Convert the model (note: we are converting the model directly from the \"concrete_func\" variable)\n",
    "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Let's save the model as \"model_3.tflite\"\n",
    "tflite_model3_path = os.path.join(os.getcwd(),'saved_tflite_models','model_3.tflite')\n",
    "with open(tflite_model3_path, 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also convert models using the [command line tool](https://www.tensorflow.org/lite/convert#command_line_tool_) (but the recommended way of doing this is using the Python API above).\n",
    "\n",
    "[Optimization](https://www.tensorflow.org/lite/performance/model_optimization) can be applied when converting models (but the converter applies some default optimization for us).\n",
    "\n",
    "Finally, we can load the TFLite models and run [inference](https://www.tensorflow.org/lite/guide/inference) on various edge devices, including various mobile phones, microcomputers and microcontrollers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and run a model in Python\n",
    "\n",
    "https://www.tensorflow.org/lite/guide/inference\n",
    "\n",
    "The Python API for running an inference is provided in the [tf.lite module](https://www.tensorflow.org/lite/api_docs/python/tf/lite). From which, you mostly need only [tf.lite.Interpreter](https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter) to load a model and run an inference.\n",
    "\n",
    "The following example shows how to use the Python interpreter to load a `.tflite` file and run inference with random input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.482947]]\n"
     ]
    }
   ],
   "source": [
    "# Load the TFLite model and allocate tensors.\n",
    "# Let's load the TFLite model we saved to \"./saved_tflite_models/model_2.tflite\" above.\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model2_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "\n",
    "# \"set_tensor\" sets the data at the model's input.\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# \"invoke()\" sends the data through the model.\n",
    "interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "# Let's get the output from the model and print it.\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to loading the model as a pre-converted `.tflite` file, you can combine your code with the [TensorFlow Lite Converter Python API](https://www.tensorflow.org/lite/models/convert) ([tf.lite.TFLiteConverter](https://www.tensorflow.org/lite/api_docs/python/tf/lite/TFLiteConverter)), allowing you to convert your TensorFlow model into the TensorFlow Lite format and then run inference.\n",
    "\n",
    "This example contains a complete workflow:\n",
    "\n",
    "1. Create a model using the `tf.keras.models.Sequential` API.\n",
    "2. Train the model.\n",
    "3. Convert the model to a TFLite model with the `tf.lite.TFLiteConverter`.\n",
    "4. Load the model and run inference with the  `tf.lite.Interpreter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 3.4925\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.3787\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.2549\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.1357\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.0200\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\gabpat\\AppData\\Local\\Temp\\tmph2n99psa\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\gabpat\\AppData\\Local\\Temp\\tmph2n99psa\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03515544]]\n"
     ]
    }
   ],
   "source": [
    "# Create a model using high-level tf.keras.* APIs\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units=1, input_shape=[1], name='input_layer'),\n",
    "    tf.keras.layers.Dense(units=16, activation='relu', name='hidden_layer'),\n",
    "    tf.keras.layers.Dense(units=1, name='output_layer')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(x=[-1, 0, 1], y=[-3, -1, 1], epochs=5)\n",
    "\n",
    "# Convert to TFLite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "\n",
    "# \"set_tensor\" sets the data at the model's input.\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# \"invoke()\" sends the data through the model.\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output from the model and print it.\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "a8159fd93c66e7d8d0e5af4b494b7ac885c17c604ffb25abe09796ec175b3861"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
